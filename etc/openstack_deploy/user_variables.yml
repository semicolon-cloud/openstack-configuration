---
# Copyright 2014, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

###
### This file contains commonly used overrides for convenience. Please inspect
### the defaults for each role to find additional override options.
###

## Debug and Verbose options.
debug: false

## Set the service setup host
# The default is to use localhost (the deploy host where ansible runs),
# but any other host can be used. If using an alternative host with all
# required libraries in a venv (eg: the utility container) then the
# python interpreter needs to be set. If it is not, the default is to
# the system python interpreter.
# If you wish to use the first utility container in the inventory for
# all service setup tasks, uncomment the following.
#
#openstack_service_setup_host: "{{ groups['utility_all'][0] }}"
#openstack_service_setup_host_python_interpreter: "/openstack/venvs/utility-{{ openstack_release }}/bin/python"

## Installation method for OpenStack services
# Default option (source) is to install the OpenStack services using PIP
# packages. An alternative method (distro) is to use the distribution cloud
# repositories to install OpenStack using distribution packages
install_method: source

## Common Glance Overrides
# Set glance_default_store to "swift" if using Cloud Files backend
# or "rbd" if using ceph backend; the latter will trigger ceph to get
# installed on glance. If using a file store, a shared file store is
# recommended. See the OpenStack-Ansible install guide and the OpenStack
# documentation for more details.
# Note that "swift" is automatically set as the default back-end if there
# are any swift hosts in the environment. Use this setting to override
# this automation if you wish for a different default back-end.
# glance_default_store: file

## Ceph pool name for Glance to use
# glance_rbd_store_pool: images
# glance_rbd_store_chunk_size: 8

## Common Nova Overrides
# When nova_libvirt_images_rbd_pool is defined, ceph will be installed on nova
# hosts.
# nova_libvirt_images_rbd_pool: vms

# If you wish to change the dhcp_domain configured for both nova and neutron
# dhcp_domain: openstacklocal

## Common Glance Overrides when using a Swift back-end
# By default when 'glance_default_store' is set to 'swift' the playbooks will
# expect to use the Swift back-end that is configured in the same inventory.
# If the Swift back-end is not in the same inventory (ie it is already setup
# through some other means) then these settings should be used.
#
# NOTE: Ensure that the auth version matches your authentication endpoint.
#
# NOTE: If the password for glance_swift_store_key contains a dollar sign ($),
# it must be escaped with an additional dollar sign ($$), not a backslash. For
# example, a password of "super$ecure" would need to be entered as
# "super$$ecure" below.  See Launchpad Bug #1259729 for more details.
#
# glance_swift_store_auth_version: 3
# glance_swift_store_auth_address: "https://some.auth.url.com"
# glance_swift_store_user: "OPENSTACK_TENANT_ID:OPENSTACK_USER_NAME"
# glance_swift_store_key: "OPENSTACK_USER_PASSWORD"
# glance_swift_store_container: "NAME_OF_SWIFT_CONTAINER"
# glance_swift_store_region: "NAME_OF_REGION"

## Common Ceph Overrides
# ceph_mons:
#   - 10.16.5.40
#   - 10.16.5.41
#   - 10.16.5.42

## Custom Ceph Configuration File (ceph.conf)
# By default, your deployment host will connect to one of the mons defined above to
# obtain a copy of your cluster's ceph.conf.  If you prefer, uncomment ceph_conf_file
# and customise to avoid ceph.conf being copied from a mon.
# ceph_conf_file: |
#   [global]
#   fsid = 00000000-1111-2222-3333-444444444444
#   mon_initial_members = mon1.example.local,mon2.example.local,mon3.example.local
#   mon_host = 10.16.5.40,10.16.5.41,10.16.5.42
#   # optionally, you can use this construct to avoid defining this list twice:
#   # mon_host = {{ ceph_mons|join(',') }}
#   auth_cluster_required = cephx
#   auth_service_required = cephx


# By default, openstack-ansible configures all OpenStack services to talk to
# RabbitMQ over encrypted connections on port 5671. To opt-out of this default,
# set the rabbitmq_use_ssl variable to 'false'. The default setting of 'true'
# is highly recommended for securing the contents of RabbitMQ messages.
# rabbitmq_use_ssl: false

# RabbitMQ management plugin is enabled by default, the guest user has been
# removed for security reasons and a new userid 'monitoring' has been created
# with the 'monitoring' user tag. In order to modify the userid, uncomment the
# following and change 'monitoring' to your userid of choice.
# rabbitmq_monitoring_userid: monitoring


## Additional pinning generator that will allow for more packages to be pinned as you see fit.
## All pins allow for package and versions to be defined. Be careful using this as versions
## are always subject to change and updates regarding security will become your problem from this
## point on. Pinning can be done based on a package version, release, or origin. Use "*" in the
## package name to indicate that you want to pin all package to a particular constraint.
# apt_pinned_packages:
#   - { package: "lxc", version: "1.0.7-0ubuntu0.1" }
#   - { package: "libvirt-bin", version: "1.2.2-0ubuntu13.1.9" }
#   - { package: "rabbitmq-server", origin: "www.rabbitmq.com" }
#   - { package: "*", release: "MariaDB" }


## Environment variable settings
# This allows users to specify the additional environment variables to be set
# which is useful in setting where you working behind a proxy. If working behind
# a proxy It's important to always specify the scheme as "http://". This is what
# the underlying python libraries will handle best. This proxy information will be
# placed both on the hosts and inside the containers.

## Example environment variable setup:
## This is used by apt-cacher-ng to download apt packages:
# proxy_env_url: http://username:pa$$w0rd@10.10.10.9:9000/

## (1) This sets up a permanent environment, used during and after deployment:
# no_proxy_env: "localhost,127.0.0.1,{{ internal_lb_vip_address }},{{ external_lb_vip_address }},{% for host in groups['all_containers'] %}{{ hostvars[host]['container_address'] }}{% if not loop.last %},{% endif %}{% endfor %}"
# global_environment_variables:
#   HTTP_PROXY: "{{ proxy_env_url }}"
#   HTTPS_PROXY: "{{ proxy_env_url }}"
#   NO_PROXY: "{{ no_proxy_env }}"
#   http_proxy: "{{ proxy_env_url }}"
#   https_proxy: "{{ proxy_env_url }}"
#   no_proxy: "{{ no_proxy_env }}"
#
## (2) This is applied only during deployment, nothing is left after deployment is complete:
# deployment_environment_variables:
#   http_proxy: "{{ proxy_env_url }}"
#   https_proxy: "{{ proxy_env_url }}"
#   no_proxy: "localhost,127.0.0.1,{{ internal_lb_vip_address }},{{ external_lb_vip_address }},{% for host in groups['keystone_all'] %}{{ hostvars[host]['container_address'] }}{% if not loop.last %},{% endif %}{% endfor %}"


## SSH connection wait time
# If an increased delay for the ssh connection check is desired,
# uncomment this variable and set it appropriately.
#ssh_delay: 5


## HAProxy and keepalived
# All the previous variables are used inside a var, in the group vars.
# You can override the current keepalived definition (see
# group_vars/all/keepalived.yml) in your user space if necessary.
#
# Uncomment this to disable keepalived installation (cf. documentation)
# haproxy_use_keepalived: False
#
# HAProxy Keepalived configuration (cf. documentation)
# Make sure that this is set correctly according to the CIDR used for your
# internal and external addresses.
# haproxy_keepalived_external_vip_cidr: "{{external_lb_vip_address}}/32"
# haproxy_keepalived_internal_vip_cidr: "{{internal_lb_vip_address}}/32"
# haproxy_keepalived_external_interface:
# haproxy_keepalived_internal_interface:

# Defines the default VRRP id used for keepalived with haproxy.
# Overwrite it to your value to make sure you don't overlap
# with existing VRRPs id on your network. Default is 10 for the external and 11 for the
# internal VRRPs
# haproxy_keepalived_external_virtual_router_id:
# haproxy_keepalived_internal_virtual_router_id:

# Defines the VRRP master/backup priority. Defaults respectively to 100 and 20
# haproxy_keepalived_priority_master:
# haproxy_keepalived_priority_backup:

# Keepalived default IP address used to check its alive status (IPv4 only)
# keepalived_external_ping_address: "193.0.14.129"
# keepalived_internal_ping_address: "193.0.14.129"
neutron_plugin_type: ml2.ovn

neutron_plugin_base:
  - ovn-router
  - vpnaas

openstack_host_specific_kernel_modules:
   - { name: "ebtables", pattern: "CONFIG_BRIDGE_NF_EBTABLES=", group: "network_hosts" }
   - { name: "af_key", pattern: "CONFIG_NET_KEY=", group: "network_hosts" }
   - { name: "ah4", pattern: "CONFIG_INET_AH=", group: "network_hosts" }
   - { name: "ipcomp", pattern: "CONFIG_INET_IPCOMP=", group: "network_hosts" }

neutron_ml2_drivers_type: "vlan,local,geneve,flat"

zun_virt_type: lxd

user_requirements_git_url: https://raw.githubusercontent.com/semicolon-cloud/openstack-requirements/stable/2023.1-better/upper-constraints.txt
# Dude
requirements_git_install_branch: e3c5bff0b26ffc6388ce0c72388b1b9b41f7f2e8
zun_kuryr_system_user_name: root
zun_kuryr_system_group_name: root

haproxy_ssl_letsencrypt_enable: true
haproxy_ssl_letsencrypt_email: cyoung06@naver.com
haproxy_ssl_letsencrypt_domains: 
- cloud.seda.club
- cloudformation.cloud.seda.club
- compute.cloud.seda.club
- container.cloud.seda.club
- identity.cloud.seda.club
- image.cloud.seda.club
- network.cloud.seda.club
- novnc.compute.cloud.seda.club
- orchestration.cloud.seda.club
- placement.cloud.seda.club
- serialconsole.compute.cloud.seda.club
- spice.compute.cloud.seda.club
- volume.cloud.seda.club
- ws.container.cloud.seda.club
- designate.cloud.seda.club
- swift.cloud.seda.club
- grafana.admin.cloud.seda.club
- kibana.admin.cloud.seda.club
- accounts.seda.club

haproxy_security_headers_csp_report_only: false
haproxy_horizon_csp: >
  http-response set-header {{ haproxy_security_headers_csp_report_only | ternary('Content-Security-Policy-Report-Only', 'Content-Security-Policy') }} "
  default-src 'self';
  frame-ancestors 'self';
  form-action 'self';
  upgrade-insecure-requests;
  style-src 'self' 'unsafe-inline';
  script-src 'self' 'unsafe-inline' 'unsafe-eval';
  child-src 'self' spice.compute.cloud.seda.club novnc.compute.cloud.seda.club serialconsole.compute.cloud.seda.club;
  frame-src 'self' spice.compute.cloud.seda.club novnc.compute.cloud.seda.club serialconsole.compute.cloud.seda.club;
  connect-src 'self' cloud.seda.club:* wss://{{ external_lb_vip_address }}:{{ ironic_console_port }} wss://ws.container.cloud.seda.club;
  img-src 'self' data:;
  worker-src blob:;
  "

placement_service_publicurl: "https://placement.cloud.seda.club"
zun_service_publicuri: "https://container.cloud.seda.club"
zun_wsproxy_base_uri: "wss://ws.container.cloud.seda.club"

nova_service_publicuri: "https://compute.cloud.seda.club"

nova_spice_html5proxy_base_uri: "https://spice.compute.cloud.seda.club"
nova_novncproxy_base_uri: "https://novnc.compute.cloud.seda.club"
nova_serialconsoleproxy_base_uri: "ws://serialconsole.compute.cloud.seda.club"

neutron_service_publicuri: "https://network.cloud.seda.club"
keystone_service_publicuri: "https://identity.cloud.seda.club"
heat_service_publicuri: "https://orchestration.cloud.seda.club"
heat_cfn_service_publicuri: "https://cloudformation.cloud.seda.club"
heat_waitcondition_server_uri: "https://cloudformation.cloud.seda.club"
heat_metadata_server_url: "https://cloudformation.cloud.seda.club"
glance_service_publicuri: "https://image.cloud.seda.club"

cinder_service_publicuri: "https://volume.cloud.seda.club"
cinder_service_v2_publicuri: "https://volume.cloud.seda.club"
cinder_service_v3_publicuri: "https://volume.cloud.seda.club"


designate_service_publicuri: "https://designate.cloud.seda.club"

swift_service_publicurl: "https://swift.cloud.seda.club/v1/AUTH_%(tenant_id)s"

profiler_overrides: &os_profiler
  profiler:
    enabled: true
    trace_sqlalchemy: true
    hmac_keys: "{{ profiler_token }}"  # This needs to be set consistently throughout the deployment
    connection_string: "Elasticsearch://{{ internal_lb_vip_address }}:9201"
    es_doc_type: "notification"
    es_scroll_time: "2m"
    es_scroll_size: "10000"
    filter_error_trace: "false"

aodh_aodh_conf_overrides: *os_profiler
barbican_config_overrides: *os_profiler
ceilometer_ceilometer_conf_overrides: *os_profiler
cinder_cinder_conf_overrides: *os_profiler
designate_designate_conf_overrides: *os_profiler
glance_glance_api_conf_overrides: *os_profiler
gnocchi_conf_overrides: *os_profiler
heat_heat_conf_overrides: *os_profiler
horizon_config_overrides: *os_profiler
ironic_ironic_conf_overrides: *os_profiler
keystone_keystone_conf_overrides: *os_profiler
magnum_config_overrides: *os_profiler
neutron_neutron_conf_overrides: *os_profiler
nova_nova_conf_overrides: *os_profiler
octavia_octavia_conf_overrides: *os_profiler
rally_config_overrides: *os_profiler
sahara_conf_overrides: *os_profiler
swift_swift_conf_overrides: *os_profiler
tacker_tacker_conf_overrides: *os_profiler
trove_config_overrides: *os_profiler

zun_zun_conf_overrides:
  websocket_proxy:
    allowed_origins:
    - 'ws.container.cloud.seda.club'
    - 'cloud.seda.club'
  <<: *os_profiler

designate_pools_yaml:
  - name: "internal"
    description: Internal Schoolnet Pool
    attributes:
      facing: internal
    ns_records:
      - hostname: ns1.seda.club.
        priority: 1
    nameservers:
      - host: 10.244.104.50
        port: 53
    targets:
      - type: bind9
        description: BIND9 Server
        masters:
          - host: 172.29.237.134
            port: 5354
          - host: 172.29.237.228
            port: 5354
          - host: 172.29.237.238
            port: 5354
        options:
          host: 172.29.236.1
          port: 53
          rndc_host: 172.29.236.1
          rndc_key_file: /etc/designate/rndc.key
          rndc_port: 953
          view: school
  - name: "external"
    description: External Schoolnet Pool
    attributes:
      facing: external
    ns_records:
      - hostname: ns1.seda.club.
        priority: 1
    nameservers:
      - host: external.seda.club
        port: 53
    targets:
      - type: bind9
        description: BIND9 Server
        masters:
          - host: 172.29.237.134
            port: 5354
          - host: 172.29.237.228
            port: 5354
          - host: 172.29.237.238
            port: 5354
        options:
          host: 172.29.236.1
          port: 53
          rndc_host: 172.29.236.1
          rndc_key_file: /etc/designate/rndc2.key
          rndc_port: 953
          view: rest

## rndc keys for authenticating with bind9
# define this to create as many key files as are required
designate_rndc_keys:
  - name: "rndc-key"
    file: /etc/designate/rndc.key
    algorithm: "hmac-sha256"
    secret: "{{ sedaclub_rndc_key }}"
  - name: "rndc-key2"
    file: /etc/designate/rndc2.key
    algorithm: "hmac-sha256"
    secret: "{{ remote_sedaclub_rndc_key }}"

swift_allow_all_users: true

glance_default_store: swift
glance_swift_store_auth_address: '{{ keystone_service_internalurl }}'
glance_swift_store_container: glance_images
glance_swift_store_endpoint_type: internalURL
glance_swift_store_key: '{{ glance_service_password }}'
glance_swift_store_region: RegionOne
glance_swift_store_user: 'service:glance'

nova_virt_types:
  lxc:
    nova_compute_driver: libvirt.LibvirtDriver
    nova_reserved_host_memory_mb: 0
    nova_scheduler_tracks_instance_changes: False
nova_virt_type: lxc

nova_supported_virt_types:
  - lxc

nova_compute_lxc_packages:
  - libvirt-daemon
  - libvirt-daemon-system
  - ovmf
  - python3-libvirt
  - qemu-efi
  - qemu-utils
  - qemu-user
  - qemu-block-extra
  - "{{ (ansible_facts['architecture'] == 'x86_64') | ternary('qemu-system-x86', 'qemu-system-arm') }}"
  - qemu-system-misc
  - ipxe-qemu
  - libvirt-daemon-driver-lxc

nova_package_list: |-
  {% set packages = nova_distro_packages %}
  {% if nova_services['nova-compute']['group'] in group_names %}
  {%   set _ = packages.extend(nova_compute_packages) %}
  {%   if nova_virt_type in ['kvm', 'qemu'] %}
  {%     set _ = packages.extend(nova_compute_kvm_distro_packages) %}
  {%     if nova_compute_ksm_enabled %}
  {%       set _ = packages.extend(nova_compute_ksm_packages) %}
  {%     endif %}
  {%   endif %}
  {%   if nova_virt_type in ['lxc'] %}
  {%     set _ = packages.extend(nova_compute_lxc_packages) %}
  {%   endif %}
  {%   if nova_barbican_enabled | bool %}
  {%     set _ = packages.extend(nova_compute_barbican_distro_packages) %}
  {%   endif %}
  {% endif %}
  {% if nova_oslomsg_amqp1_enabled | bool %}
  {%   set _ = packages.extend(nova_compute_oslomsg_amqp1_distro_packages) %}
  {% endif %}
  {{ packages }}


# don't make too much workers
zun_osapi_compute_workers: 3
zun_conductor_workers: 3
zun_metadata_workers: 3
zun_wsgi_processes_max: 3


swift_account_server_replicator_workers: 3
swift_server_replicator_workers: 3
swift_object_replicator_workers: 3
swift_account_server_workers: 3
swift_container_server_workers: 3
swift_object_server_workers: 3
swift_proxy_server_workers_max: 3

placement_wsgi_processes_max: 3


nova_wsgi_processes_max: 3
nova_conductor_workers: 3
nova_scheduler_workers: 3

neutron_api_workers: 3
neutron_rpc_workers_max: 3
neutron_wsgi_processes_max: 3

keystone_wsgi_processes_max: 3

# heat_engine_workers: 4
# heat_api_workers: 4 heat is reasonable

glance_api_workers: 3
glance_wsgi_processes_max: 3


cinder_wsgi_processes_max: 3

cinder_osapi_volume_workers_max: 3


nova_console_type: 'serialconsole'

nova_cpu_allocation_ratio: 16

# NodeExporter
galera_additional_users:
  - name: "exporter"
    host: '%'
    password: "{{ prometheus_mysqld_exporter_galera_password }}"
    priv: '*.*:PROCESS,REPLICATION CLIENT,SELECT,SLAVE MONITOR'
    resource_limits:
      MAX_USER_CONNECTIONS: 3
    check_hostname: false
    state: present
